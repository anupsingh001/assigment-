{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iViAHMOE0max"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Boosting in Machine Learning? Explain how it improves weak learners.\n",
        "Ans- Boosting is an ensemble learning technique that combines multiple weak learners (usually simple models like decision stumps‚Äîshallow decision trees with only one split) to form a strong learner with much better predictive performance.\n",
        "A weak learner is a model that performs just slightly better than random guessing (e.g., 51‚Äì60% accuracy).\n",
        "\n",
        "\n",
        "Boosting trains these weak learners sequentially, where each new learner focuses more on the mistakes made by the previous ones.\n",
        "\n",
        "\n",
        "The final prediction is obtained by weighted voting (for classification) or weighted averaging (for regression) of all weak learners.\n",
        "\n",
        "\n",
        "\n",
        "The key idea is that boosting turns a group of weak learners into a strong learner through an iterative process:\n",
        "Initialize Model\n",
        "\n",
        "\n",
        "Start with equal weights for all training samples.\n",
        "\n",
        "\n",
        "Train the first weak learner (e.g., decision stump).\n",
        "\n",
        "\n",
        "Identify Errors\n",
        "\n",
        "\n",
        "Check which samples were misclassified by the weak learner.\n",
        "\n",
        "\n",
        "Adjust Weights\n",
        "\n",
        "\n",
        "Increase the weights of misclassified samples so that the next learner pays more attention to the \"hard-to-classify\" cases.\n",
        "\n",
        "\n",
        "Decrease the weights of correctly classified samples.\n",
        "\n",
        "\n",
        "Train Next Learner\n",
        "\n",
        "\n",
        "Fit the next weak learner on the re-weighted dataset.\n",
        "\n",
        "\n",
        "Repeat the process for multiple rounds.\n",
        "\n",
        "\n",
        "Combine Learners\n",
        "\n",
        "\n",
        "Final model is a weighted sum of all weak learners, where more accurate learners get higher weights.\n"
      ],
      "metadata": {
        "id": "2_LVdMxd00cL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the difference between AdaBoost and Gradient Boosting in terms of how models are trained?\n",
        "Ans- 1. AdaBoost (Adaptive Boosting)\n",
        "Idea: Focus on misclassified samples.\n",
        "\n",
        "\n",
        "How it works:\n",
        "\n",
        "\n",
        "Start with equal weights for all training data points.\n",
        "\n",
        "\n",
        "Train a weak learner (usually a decision stump).\n",
        "\n",
        "\n",
        "Increase weights of misclassified samples so the next learner pays more attention to them.\n",
        "\n",
        "\n",
        "Repeat the process for multiple learners.\n",
        "\n",
        "\n",
        "Final prediction is a weighted vote of all learners.\n",
        "\n",
        "\n",
        "üëâ Key point: AdaBoost updates data weights after each iteration.\n",
        "\n",
        "2. Gradient Boosting\n",
        "Idea: Reduce errors using gradient descent.\n",
        "\n",
        "\n",
        "How it works:\n",
        "\n",
        "\n",
        "Start with an initial prediction (e.g., mean for regression, log-odds for classification).\n",
        "\n",
        "\n",
        "Compute the residuals (errors) = actual ‚Äì predicted.\n",
        "\n",
        "\n",
        "Train the next weak learner to predict these residuals.\n",
        "\n",
        "\n",
        "Update the overall model by adding the new learner‚Äôs contribution, scaled by a learning rate.\n",
        "\n",
        "\n",
        "Repeat for many learners.\n",
        "\n",
        "\n",
        "üëâ Key point: Gradient Boosting optimizes a loss function directly by fitting learners to residuals (using gradient descent).\n",
        "\n",
        "Main Differences Between AdaBoost and Gradient Boosting\n",
        "Aspect\n",
        "AdaBoost\n",
        "Gradient Boosting\n",
        "Focus\n",
        "Adjusts sample weights based on misclassifications\n",
        "Fits weak learners to residual errors using gradient descent\n",
        "Training\n",
        "Emphasizes ‚Äúhard‚Äù cases (misclassified points)\n",
        "Sequentially reduces overall loss function\n",
        "Loss Function\n",
        "Implicitly uses exponential loss\n",
        "Can use different losses (MSE, MAE, logistic loss, etc.)\n",
        "Weight Update\n",
        "Increases weights of misclassified samples\n",
        "Updates model by adding a new learner‚Äôs predictions\n",
        "Base Learners\n",
        "Commonly decision stumps (1-level trees)\n",
        "Usually deeper decision trees (e.g., 3‚Äì8 levels)\n",
        "Interpretation\n",
        "\n",
        "\n",
        "\n",
        "Re-weights data points\n",
        "Re-weights predictions/residuals\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wtnsd1t702L6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: How does regularization help in XGBoost?\n",
        "Answer:\n",
        "\n",
        "XGBoost (Extreme Gradient Boosting) includes regularization terms in its objective function, unlike traditional Gradient Boosting.\n",
        "\n",
        "Regularization in XGBoost\n",
        "\n",
        "Objective function = Loss Function + Regularization Term\n",
        "\n",
        "ùëÇ\n",
        "ùëè\n",
        "ùëó\n",
        "=\n",
        "‚àë\n",
        "ùëñ\n",
        "ùêø\n",
        "(\n",
        "ùë¶\n",
        "ùëñ\n",
        ",\n",
        "ùë¶\n",
        "^\n",
        "ùëñ\n",
        ")\n",
        "+\n",
        "Œ©\n",
        "(\n",
        "ùëì\n",
        "ùë°\n",
        ")\n",
        "Obj=\n",
        "i\n",
        "‚àë\n",
        "\t‚Äã\n",
        "\n",
        "L(y\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        ",\n",
        "y\n",
        "^\n",
        "\t‚Äã\n",
        "\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        ")+Œ©(f\n",
        "t\n",
        "\t‚Äã\n",
        "\n",
        ")\n",
        "\n",
        "where\n",
        "\n",
        "Œ©\n",
        "(\n",
        "ùëì\n",
        "ùë°\n",
        ")\n",
        "=\n",
        "ùõæ\n",
        "ùëá\n",
        "+\n",
        "1\n",
        "2\n",
        "ùúÜ\n",
        "‚àë\n",
        "ùëó\n",
        "ùë§\n",
        "ùëó\n",
        "2\n",
        "Œ©(f\n",
        "t\n",
        "\t‚Äã\n",
        "\n",
        ")=Œ≥T+\n",
        "2\n",
        "1\n",
        "\t‚Äã\n",
        "\n",
        "Œª‚àë\n",
        "j\n",
        "\t‚Äã\n",
        "\n",
        "w\n",
        "j\n",
        "2\n",
        "\t‚Äã\n",
        "\n",
        "\n",
        "ùëá\n",
        "T: Number of leaves in the tree (penalty on complexity).\n",
        "\n",
        "ùë§\n",
        "ùëó\n",
        "w\n",
        "j\n",
        "\t‚Äã\n",
        "\n",
        ": Leaf weights.\n",
        "\n",
        "ùúÜ\n",
        "Œª: L2 regularization (prevents large weights).\n",
        "\n",
        "ùõæ\n",
        "Œ≥: Minimum loss reduction required to make a further split.\n",
        "\n",
        "Benefits of Regularization in XGBoost:\n",
        "\n",
        "Prevents overfitting by discouraging overly complex trees.\n",
        "\n",
        "Encourages sparser trees (prunes unnecessary splits).\n",
        "\n",
        "Improves generalization by controlling model complexity.\n",
        "\n",
        "Makes the model more robust and stable.\n",
        "\n",
        "üëâ In short: Regularization in XGBoost helps balance model complexity with accuracy, leading to better generalization and reduced overfitting.\n",
        "\n",
        "Question 4: Why is CatBoost considered efficient for handling categorical data?\n",
        "Answer:\n",
        "\n",
        "CatBoost (by Yandex) is designed specifically to handle categorical features efficiently without extensive preprocessing (like one-hot encoding).\n",
        "\n",
        "Traditional ML issue:\n",
        "\n",
        "Most algorithms can‚Äôt handle categorical data directly.\n",
        "\n",
        "Requires label encoding or one-hot encoding, which increases dimensionality and may introduce bias.\n",
        "\n",
        "CatBoost Solution:\n",
        "\n",
        "Uses a technique called Ordered Target Statistics (Ordered Encoding):\n",
        "\n",
        "Instead of replacing categories with arbitrary numbers or dummies, CatBoost replaces them with statistics based on the target variable (like mean target value per category).\n",
        "\n",
        "To avoid target leakage, it uses ordered boosting, where encoding is based only on past data in the permutation order.\n",
        "\n",
        "Efficiency Advantages:\n",
        "\n",
        "Handles categorical features directly (no manual encoding needed).\n",
        "\n",
        "Prevents target leakage with ordered encoding.\n",
        "\n",
        "Reduces dimensionality explosion (no need for thousands of one-hot columns).\n",
        "\n",
        "Works well with high-cardinality features (e.g., ZIP codes, product IDs).\n",
        "\n",
        "üëâ In short: CatBoost is efficient for categorical data because it directly encodes categories using target-based statistics with ordered boosting, avoiding manual preprocessing and overfitting issues."
      ],
      "metadata": {
        "id": "cwBKoP1D1KlA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What are some real-world applications where boosting techniques are\n",
        "preferred over bagging methods?\n",
        "Boosting vs. Bagging in Practice\n",
        "\n",
        "Bagging (Bootstrap Aggregating):\n",
        "\n",
        "Reduces variance by training models in parallel on bootstrapped samples (e.g., Random Forest).\n",
        "\n",
        "Good for high-variance, unstable models (like deep decision trees).\n",
        "\n",
        "Boosting:\n",
        "\n",
        "Reduces bias and variance by training weak learners sequentially, each focusing on previous errors (e.g., AdaBoost, Gradient Boosting, XGBoost, LightGBM, CatBoost).\n",
        "\n",
        "Often achieves higher accuracy but at the cost of more computation.\n",
        "\n",
        "Real-World Applications Where Boosting is Preferred\n",
        "\n",
        "Credit Scoring & Fraud Detection (Finance)\n",
        "\n",
        "Boosting (especially XGBoost & LightGBM) is widely used in banking and fintech.\n",
        "\n",
        "Can capture complex non-linear patterns in transaction data, outperforming Random Forests.\n",
        "\n",
        "Example: Predicting loan defaults, detecting fraudulent transactions.\n",
        "\n",
        "Customer Churn Prediction (Telecom, SaaS)\n",
        "\n",
        "Boosting helps in identifying subtle patterns in customer behavior data.\n",
        "\n",
        "Preferred because misclassification costs are high (losing a valuable customer).\n",
        "\n",
        "Search Ranking & Recommendation Systems (E-commerce, Tech)\n",
        "\n",
        "Companies like Amazon, Netflix, YouTube use Gradient Boosting for ranking/recommendation.\n",
        "\n",
        "Example: XGBoost was originally built for Kaggle competitions like the Netflix Prize.\n",
        "\n",
        "Medical Diagnosis & Bioinformatics (Healthcare)\n",
        "\n",
        "Boosting models are strong at handling imbalanced datasets (rare diseases).\n",
        "\n",
        "Example: Predicting cancer risk, classifying genetic data, early disease detection.\n",
        "\n",
        "Insurance Risk Modeling\n",
        "\n",
        "Boosting is used for actuarial predictions (e.g., accident probability, claim likelihood).\n",
        "\n",
        "Works better than bagging since risks involve complex, subtle interactions.\n",
        "\n",
        "Natural Language Processing (NLP)\n",
        "\n",
        "Before deep learning dominated NLP, boosted trees were widely used for text classification, spam filtering, and sentiment analysis.\n",
        "\n",
        "Still useful when datasets are structured + categorical.\n",
        "\n",
        "Kaggle Competitions & Industry Benchmarks\n",
        "\n",
        "In most tabular data problems, boosting (XGBoost, LightGBM, CatBoost) consistently beats Random Forests.\n",
        "\n",
        "Preferred when the goal is maximum accuracy.\n",
        "\n",
        "‚úÖ In short:\n",
        "Boosting is preferred over bagging in domains where:\n",
        "\n",
        "High accuracy is critical.\n",
        "\n",
        "Data has complex, subtle patterns.\n",
        "\n",
        "Misclassification costs are high.\n",
        "\n",
        "Datasets are structured/tabular (finance, healthcare, customer analytics).\n"
      ],
      "metadata": {
        "id": "OrmHK5TE1gwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 6: AdaBoost on Breast Cancer Dataset\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Initialize AdaBoost Classifier\n",
        "# Using default base estimator (DecisionTreeClassifier with max_depth=1)\n",
        "model = AdaBoostClassifier(n_estimators=100, learning_rate=1.0, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"AdaBoost Classifier Accuracy on Breast Cancer dataset: {:.2f}%\".format(accuracy * 100))\n"
      ],
      "metadata": {
        "id": "JZVrTf9Y2Rt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7 Solution\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Split dataset into train and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize Gradient Boosting Regressor\n",
        "gbr = GradientBoostingRegressor(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "gbr.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = gbr.predict(X_test)\n",
        "\n",
        "# Evaluate performance using R-squared score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"R-squared Score:\", r2)\n",
        ""
      ],
      "metadata": {
        "id": "0geQv2px2sa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8 Solution\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Initialize XGBoost Classifier\n",
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "\n",
        "# Define parameter grid for learning_rate tuning\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3]\n",
        "}\n",
        "\n",
        "# Perform GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=xgb,\n",
        "    param_grid=param_grid,\n",
        "    scoring='accuracy\n"
      ],
      "metadata": {
        "id": "GJ6h8QK82vED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9 Solution\n",
        "\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Initialize and train CatBoost Classifier\n",
        "model = CatBoostClassifier(\n",
        "    iterations=200,\n",
        "    learning_rate=0.05,\n",
        "    depth=6,\n",
        "    verbose=0,        # suppress training logs\n",
        "    random_seed=42\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {acc:.4f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot confusion matrix with seaborn\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=data.target_names,\n",
        "            yticklabels=data.target_names)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix - CatBoost Classifier\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qUB1DIAH23sI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 10: You're working for a FinTech company trying to predict loan default using\n",
        "customer demographics and transaction behavior.\n",
        "The dataset is imbalanced, contains missing values, and has both numeric and\n",
        "categorical features.\n",
        "Describe your step-by-step data science pipeline using boosting techniques:\n",
        "‚óè Data preprocessing & handling missing/categorical values\n",
        "‚óè Choice between AdaBoost, XGBoost, or CatBoost\n",
        "‚óè Hyperparameter tuning strategy\n",
        "‚óè Evaluation metrics you'd choose and why\n",
        "‚óè How the business would benefit from your model\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "ans- Step-by-Step Solution\n",
        "1. Data Preprocessing\n",
        "\n",
        "Handle missing values:\n",
        "\n",
        "Numeric: impute with median.\n",
        "\n",
        "Categorical: impute with mode or let CatBoost handle directly.\n",
        "\n",
        "Encoding categorical variables:\n",
        "\n",
        "CatBoost natively handles categorical features (no one-hot needed).\n",
        "\n",
        "XGBoost/AdaBoost ‚Üí use OneHotEncoder or LabelEncoder.\n",
        "\n",
        "Feature scaling:\n",
        "\n",
        "Not required for tree-based models.\n",
        "\n",
        "Handle imbalance:\n",
        "\n",
        "Use SMOTE/ADASYN for oversampling.\n",
        "\n",
        "Or apply class_weight or scale_pos_weight (in XGBoost).\n",
        "\n",
        "2. Choice of Boosting Algorithm\n",
        "\n",
        "AdaBoost: Simple but weaker on high-dimensional/imbalanced data.\n",
        "\n",
        "XGBoost: Very powerful, widely used, efficient handling of imbalance via scale_pos_weight.\n",
        "\n",
        "CatBoost: Best for categorical-heavy datasets, less preprocessing needed, robust with missing values.\n",
        "\n",
        "üëâ Final choice: CatBoost (since dataset has categorical + missing values).\n",
        "\n",
        "3. Hyperparameter Tuning Strategy\n",
        "\n",
        "Use GridSearchCV or RandomizedSearchCV with stratified folds.\n",
        "\n",
        "Important hyperparameters:\n",
        "\n",
        "iterations (trees)\n",
        "\n",
        "depth (tree depth)\n",
        "\n",
        "learning_rate\n",
        "\n",
        "l2_leaf_reg (regularization)\n",
        "\n",
        "class_weights (to handle imbalance)\n",
        "\n",
        "4. Evaluation Metrics\n",
        "\n",
        "AUC-ROC ‚Üí good for imbalanced classification.\n",
        "\n",
        "F1-score ‚Üí balances precision & recall (important for loan defaults).\n",
        "\n",
        "Precision & Recall separately ‚Üí\n",
        "\n",
        "High Recall ‚Üí fewer risky loans missed.\n",
        "\n",
        "High Precision ‚Üí fewer false alarms on good customers.\n",
        "\n",
        "5. Business Benefits\n",
        "\n",
        "Reduces financial risk by identifying potential defaulters.\n",
        "\n",
        "Helps in designing better lending policies.\n",
        "\n",
        "Improves customer segmentation for targeted offers.\n",
        "\n",
        "Builds trust with stakeholders by lowering NPA (Non-Performing Assets)\n",
        "\n",
        "\n",
        "# Loan Default Prediction using CatBoost\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# ---- Step 1: Load dataset ----\n",
        "# Example: df = pd.read_csv(\"loan_data.csv\")\n",
        "# Assume target column = \"default\" (0 = no default, 1 = default)\n",
        "\n",
        "# For demonstration, creating synthetic dataset\n",
        "from sklearn.datasets import make_classification\n",
        "X, y = make_classification(n_samples=5000, n_features=15,\n",
        "                           n_informative=10, n_redundant=2,\n",
        "                           weights=[0.8, 0.2], random_state=42)\n",
        "\n",
        "df = pd.DataFrame(X, columns=[f\"feature_{i}\" for i in range(15)])\n",
        "df[\"target\"] = y\n",
        "\n",
        "# ---- Step 2: Train-test split ----\n",
        "X = df.drop(\"target\", axis=1)\n",
        "y = df[\"target\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.2,\n",
        "                                                    stratify=y,\n",
        "                                                    random_state=42)\n",
        "\n",
        "# ---- Step 3: Define CatBoost model ----\n",
        "cat_model = CatBoostClassifier(verbose=0, random_state=42)\n",
        "\n",
        "# Hyperparameter grid\n",
        "param_grid = {\n",
        "    'iterations': [200, 500],\n",
        "    'depth': [4, 6, 8],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'l2_leaf_reg': [1, 3, 5],\n",
        "    'class_weights': [[1, 4], [1, 5]]  # handle imbalance\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(cat_model, param_grid, cv=3, scoring='roc_auc', n_jobs=-1)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# ---- Step 4: Best model ----\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "y_prob = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_prob))\n"
      ],
      "metadata": {
        "id": "Udy6cqdS3P2j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}